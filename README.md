# Osaurus ğŸ¦•

Native, Apple Siliconâ€“only local LLM server. Similar to Ollama, but built on Apple's MLX for maximum performance on Mâ€‘series chips. SwiftUI app + SwiftNIO server with OpenAIâ€‘compatible endpoints.

Created by Dinoki Labs ([dinoki.ai](https://dinoki.ai)), a fully native desktop AI assistant and companion.

## Highlights

- **Native MLX runtime**: Optimized for Apple Silicon using MLX/MLXLLM
- **Apple Silicon only**: Designed and tested for Mâ€‘series Macs
- **OpenAI API compatible**: `/v1/models` and `/v1/chat/completions` (stream and nonâ€‘stream)
- **Fast token streaming**: Serverâ€‘Sent Events for lowâ€‘latency output
- **Model manager UI**: Browse, download, and manage MLX models from `mlx-community`
- **Selfâ€‘contained**: SwiftUI app with an embedded SwiftNIO HTTP server

## Requirements

- macOS 15.5+
- Apple Silicon (M1 or newer)
- Xcode 16.4+ (to build from source)

```
osaurus/
â”œâ”€â”€ Core/
â”‚   â”œâ”€â”€ AppDelegate.swift
â”‚   â””â”€â”€ osaurusApp.swift
â”œâ”€â”€ Controllers/
â”‚   â”œâ”€â”€ ServerController.swift      # NIO server lifecycle
â”‚   â””â”€â”€ ModelManager.swift          # Model discovery & downloads (Hugging Face)
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ MLXModel.swift
â”‚   â”œâ”€â”€ OpenAIAPI.swift             # OpenAIâ€‘compatible DTOs
â”‚   â”œâ”€â”€ ServerConfiguration.swift
â”‚   â””â”€â”€ ServerHealth.swift
â”œâ”€â”€ Networking/
â”‚   â”œâ”€â”€ HTTPHandler.swift           # Request parsing & routing entry
â”‚   â”œâ”€â”€ Router.swift                # Routes â†’ handlers
â”‚   â””â”€â”€ AsyncHTTPHandler.swift      # SSE streaming for chat completions
â”œâ”€â”€ Services/
â”‚   â”œâ”€â”€ MLXService.swift            # MLX loading, session caching, generation
â”‚   â””â”€â”€ SearchService.swift
â”œâ”€â”€ Theme/
â”‚   â””â”€â”€ Theme.swift
â”œâ”€â”€ Views/
â”‚   â”œâ”€â”€ Components/SimpleComponents.swift
â”‚   â”œâ”€â”€ ContentView.swift           # Start/stop server, quick controls
â”‚   â””â”€â”€ ModelDownloadView.swift     # Browse/download/manage models
â””â”€â”€ Assets.xcassets/
```

## Features

- Native MLX text generation with model session caching
- Model manager with curated suggestions (Llama, Qwen, Gemma, Mistral, etc.)
- Download sizes estimated via Hugging Face metadata
- Streaming and nonâ€‘streaming chat completions
- Health endpoint and simple status UI

## API Endpoints

- `GET /` â†’ Plain text status
- `GET /health` â†’ JSON health info
- `GET /models` and `GET /v1/models` â†’ OpenAIâ€‘compatible models list
- `POST /chat/completions` and `POST /v1/chat/completions` â†’ OpenAIâ€‘compatible chat completions

## Getting Started

### Build and run

1. Open `osaurus.xcodeproj` in Xcode 16.4+
2. Build and run the `osaurus` target
3. In the UI, pick a port (default `8080`) and press Start
4. Open the model manager to download a model (e.g., â€œLlama 3.2 3B Instruct 4bitâ€)

Models are stored by default at `~/Documents/MLXModels`. Override with the environment variable `OSU_MODELS_DIR`.

### Use the API

Base URL: `http://127.0.0.1:8080` (or your chosen port)

List models:

```bash
curl -s http://127.0.0.1:8080/v1/models | jq
```

Nonâ€‘streaming chat completion:

```bash
curl -s http://127.0.0.1:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "llama-3.2-3b-instruct-4bit",
        "messages": [{"role":"user","content":"Write a haiku about dinosaurs"}],
        "max_tokens": 200
      }'
```

Streaming chat completion (SSE):

```bash
curl -N http://127.0.0.1:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "llama-3.2-3b-instruct-4bit",
        "messages": [{"role":"user","content":"Summarize Jurassic Park in one paragraph"}],
        "stream": true
      }'
```

Tip: Model names are lowerâ€‘cased with hyphens (derived from the friendly name), for example: `Llama 3.2 3B Instruct 4bit` â†’ `llama-3.2-3b-instruct-4bit`.

### Use with OpenAI SDKs

Point your client at Osaurus and use any placeholder API key.

Python example:

```python
from openai import OpenAI

client = OpenAI(base_url="http://127.0.0.1:8080/v1", api_key="osaurus")

resp = client.chat.completions.create(
    model="llama-3.2-3b-instruct-4bit",
    messages=[{"role": "user", "content": "Hello there!"}],
)

print(resp.choices[0].message.content)
```

## Models

- Curated suggestions include Llama, Qwen, Gemma, Mistral, Phi, DeepSeek, etc. (4â€‘bit variants for speed)
- Discovery pulls from Hugging Face `mlx-community` and computes size estimates
- Required files are fetched automatically (tokenizer/config/weights)
- Change the models directory with `OSU_MODELS_DIR`

## Notes & Limitations

- Apple Silicon only (requires MLX); Intel Macs are not supported
- Localhost only, no authentication; put behind a proxy if exposing externally
- `/transcribe` endpoints are placeholders pending Whisper integration

## Dependencies

- SwiftNIO (HTTP server)
- SwiftUI/AppKit (UI)
- MLXâ€‘Swift, MLXLLM (runtime and chat session)
